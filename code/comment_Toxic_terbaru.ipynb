{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQG4MU9OH4k-",
        "outputId": "d8b5cb7d-42c3-43a2-b8a5-0e71d17c4935"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Robby\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\Robby\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "c:\\Users\\Robby\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.13.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysAeuiKyH4lU"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None )\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfLn36qoH4lW"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(r\"C:\\Users\\Robby\\Downloads\\PROJECT\\Toxic Comment Classification\\train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRZ8hk-MH4lX"
      },
      "outputs": [],
      "source": [
        "train_df.fillna(' ', inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WjDT3rIH4lY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "def clean_punctuation(text):\n",
        "    text = str(text)\n",
        "    # Case folding\n",
        "    text = text.lower()\n",
        "    # Menghapus spasi berlebih\n",
        "    text = ' '.join(text.split())\n",
        "    # substitusi kata\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"r\", \"\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    # Menghapus karakter khusus, tanda baca\n",
        "    text = re.sub(r'[-.,+\"&\\'#@;:{}`+=~/!?()]', '', text)\\\n",
        "    # memastikan tanda baca terhapus\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(\"(\\W)\",\" \",text)\n",
        "    # menghapus kata yang diapit oleh karakter s\n",
        "    text = re.sub('S*dS*s*','', text)\n",
        "\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL-TiEjfH4la"
      },
      "outputs": [],
      "source": [
        "train_df['comment_text'] = train_df['comment_text'].apply(clean_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXgaPVh6H4le"
      },
      "outputs": [],
      "source": [
        "x = train_df['comment_text'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Y4zMPhH4lg"
      },
      "outputs": [],
      "source": [
        "y = train_df['toxic'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHwpZPAeH4lh"
      },
      "outputs": [],
      "source": [
        "max_features = 20000 # hanya mengambil 20000 kata yang paling sering muncul\n",
        "max_text_length = 400 # panjang maksimum dari teks yang akan diolah max_features = 20000 # hanya mengambil 20000 kata yang paling sering muncul\n",
        "max_text_length = 400 # panjang maksimum dari teks yang akan diolah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSGgLefbH4lm"
      },
      "outputs": [],
      "source": [
        "x_tokenizer = text.Tokenizer(max_features) # membuat tokenizer, ini hanya berupa objek kosong yang akan menampung hasil tokenisasi\n",
        "\n",
        "# hanya membangun sebuah kamus kata-kata yang ada dalam teks dan menghitung seberapa sering kata muncul, secara sederhana\n",
        "# # hanya mengumpulkan statitik tentang frekeuensi kemunculan kata pada setiap teks\n",
        "x_tokenizer.fit_on_texts(list(x))\n",
        "\n",
        "# proses mengubah kata menjadi urutan token numerik berdasarkan kamus yang telah dibuat diatas\n",
        "x_tokenized = x_tokenizer.texts_to_sequences(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVGIvNufH4lo"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Menyimpan tokenizer ke dalam file pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(x_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwLqAWLfH4lo"
      },
      "outputs": [],
      "source": [
        "# disebut juga dengan padding\n",
        "# membuat sequence dengan panjang maksimum 400, dikarenakan dalam klasifikasi teks, model memerlukan input dengan panjang yang sama\n",
        "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGlmdv-oH4lq",
        "outputId": "a2f0fc68-dbe3-40ca-a38b-3bc34028ecd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ditemukan 400000 vektor kata.\n"
          ]
        }
      ],
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_index = dict()\n",
        "with open(r\"C:\\Users\\Robby\\Downloads\\PROJECT\\Toxic Comment Classification\\glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # mengambil kata\n",
        "        coef = np.asarray(values[1:], dtype='float32') # mengubah nilai koefisien menjadi float32\n",
        "        embedding_index[word] = coef\n",
        "print(f'Ditemukan {len(embedding_index)} vektor kata.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFSSZkpoH4lr"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, index in x_tokenizer.word_index.items():\n",
        "    if index > max_features - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snM4ghELH4ls"
      },
      "source": [
        "### CNN MULTICHANNEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uChS7S4MH4lu",
        "outputId": "cacd4662-4aae-4aff-c9fc-c447366a506d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 400)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 400)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 400, 100)     2000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 400, 100)     2000000     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 398, 250)     75250       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 398, 250)     75250       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 199, 250)     0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 199, 250)    0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 250)         0           ['max_pooling1d[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 250)         0           ['max_pooling1d_1[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 500)          0           ['global_max_pooling1d[0][0]',   \n",
            "                                                                  'global_max_pooling1d_1[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 250)          125250      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 250)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            251         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,276,001\n",
            "Trainable params: 2,276,001\n",
            "Non-trainable params: 2,000,000\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, concatenate\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "# Channel 1: Custom Embedding Matrix (Static)\n",
        "input1 = Input(shape=(max_text_length,))\n",
        "x1 = Embedding(max_features, embedding_dim,\n",
        "               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "               trainable=False)(input1)\n",
        "x1 = Conv1D(250, 3, activation='relu')(x1)\n",
        "x1 = MaxPooling1D()(x1)\n",
        "x1 = GlobalMaxPooling1D()(x1)\n",
        "\n",
        "# Channel 2: Trainable Embedding\n",
        "input2 = Input(shape=(max_text_length,))\n",
        "x2 = Embedding(max_features, embedding_dim, trainable=True)(input2)\n",
        "x2 = Conv1D(250, 3, activation='relu')(x2)\n",
        "x2 = MaxPooling1D()(x2)\n",
        "x2 = GlobalMaxPooling1D()(x2)\n",
        "\n",
        "# Menggabungkan kedua channel\n",
        "merged = concatenate([x1, x2])\n",
        "\n",
        "l2_reg = 0.001  # Regularisasi L2\n",
        "\n",
        "# Tambahkan Dense layers dengan regularisasi\n",
        "merged = Dense(250, activation='relu', kernel_regularizer=l2(l2_reg))(merged)\n",
        "merged = Dropout(0.5)(merged)  # Tingkatkan dropout rate\n",
        "output = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "# Membuat model\n",
        "model = Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti1zTZbEH4lv"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Menggunakan kolom 'toxic' sebagai label\n",
        "labels = train_df['toxic'].values\n",
        "\n",
        "# Menghitung bobot kelas\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "# Mengonversi bobot kelas menjadi dictionary\n",
        "class_weight_dict = {i : class_weights[i] for i in range(len(class_weights))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwSoRGBsH4lv"
      },
      "outputs": [],
      "source": [
        "x_train , x_val, y_train, y_val = train_test_split(x_train_val, y, test_size= 0.15, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw-_wN1zH4lv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001)\n",
        "callbacks_list = [early_stopping, reduce_lr]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMr_avvaH4lw"
      },
      "outputs": [],
      "source": [
        "# Jika hanya memiliki satu representasi data\n",
        "x_train_1, x_val_1, y_train, y_val = train_test_split(x_train_val, y, test_size=0.15, random_state=1)\n",
        "x_train_2, x_val_2 = x_train_1, x_val_1  # Menggunakan data yang sama untuk kedua channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAxRksGnH4lw",
        "outputId": "09d8d3bd-7b58-45bc-e847-6b97d193997b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "4239/4239 [==============================] - 824s 194ms/step - loss: 0.3094 - accuracy: 0.8967 - precision: 0.4785 - recall: 0.8767 - auc: 0.9557 - val_loss: 0.2931 - val_accuracy: 0.8854 - val_precision: 0.4541 - val_recall: 0.9328 - val_auc: 0.9708 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "4239/4239 [==============================] - 981s 232ms/step - loss: 0.1966 - accuracy: 0.9275 - precision: 0.5749 - recall: 0.9335 - auc: 0.9795 - val_loss: 0.2994 - val_accuracy: 0.8895 - val_precision: 0.4637 - val_recall: 0.9332 - val_auc: 0.9687 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "4239/4239 [==============================] - 931s 220ms/step - loss: 0.1529 - accuracy: 0.9423 - precision: 0.6311 - recall: 0.9566 - auc: 0.9875 - val_loss: 0.2183 - val_accuracy: 0.9157 - val_precision: 0.5369 - val_recall: 0.9085 - val_auc: 0.9688 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "4239/4239 [==============================] - 876s 207ms/step - loss: 0.1224 - accuracy: 0.9553 - precision: 0.6892 - recall: 0.9718 - auc: 0.9917 - val_loss: 0.1866 - val_accuracy: 0.9342 - val_precision: 0.6113 - val_recall: 0.8704 - val_auc: 0.9625 - lr: 0.0010\n",
            "Epoch 5/5\n",
            "4239/4239 [==============================] - 909s 215ms/step - loss: 0.0979 - accuracy: 0.9659 - precision: 0.7453 - recall: 0.9786 - auc: 0.9946 - val_loss: 0.1850 - val_accuracy: 0.9382 - val_precision: 0.6358 - val_recall: 0.8392 - val_auc: 0.9576 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "# Kompilasi model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
        "\n",
        "# Sekarang, lakukan fitting model\n",
        "history = model.fit(\n",
        "    [x_train, x_train], y_train,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_data=([x_val, x_val], y_val),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks_list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNlArb78H4ly",
        "outputId": "88d5a931-ae88-46a1-8b36-35f2cb779b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Akurasi Pelatihan: [0.8967006802558899, 0.9275408387184143, 0.9423084259033203, 0.9553360342979431, 0.9659306406974792]\n",
            "Akurasi Validasi: [0.8854445219039917, 0.8895387649536133, 0.9156500697135925, 0.9341577291488647, 0.938168466091156]\n"
          ]
        }
      ],
      "source": [
        "# Melihat Akurasi\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "print(\"Akurasi Pelatihan:\", train_acc)\n",
        "print(\"Akurasi Validasi:\", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhLEwJpFH4lz",
        "outputId": "6eaf2e99-a633-4cf2-d552-35a522041f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4787/4787 [==============================] - 254s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "# 1. Memuat Data Test\n",
        "test_df = pd.read_csv(r\"C:\\Users\\Robby\\Downloads\\PROJECT\\Toxic Comment Classification\\test.csv\")\n",
        "\n",
        "# 2. Membersihkan Data Test\n",
        "test_df.fillna(' ', inplace=True)\n",
        "test_df['comment_text'] = test_df['comment_text'].apply(clean_punctuation)\n",
        "\n",
        "# 3. Tokenisasi Data Test\n",
        "x_test = test_df['comment_text'].values\n",
        "x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# 4. Padding Data Test\n",
        "x_test_padded = sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)\n",
        "\n",
        "# 5. Membuat Prediksi\n",
        "predictions = model.predict([x_test_padded, x_test_padded])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ-cvi9NH4l0"
      },
      "outputs": [],
      "source": [
        "model.save(r'C:\\Users\\Robby\\Downloads\\PROJECT\\Toxic Comment Classification\\modelku.h5')  # menyimpan model Keras ke file .h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABiQdDUmH4l0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}